{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15587f4",
   "metadata": {},
   "source": [
    "# Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18502a",
   "metadata": {},
   "source": [
    "# A categorical data clustering framework on graph representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8d21a",
   "metadata": {},
   "source": [
    "# Conducted By: Liang Bai and Jiye Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a89a4",
   "metadata": {},
   "source": [
    "- Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education\n",
    "- School of Computer and Information Technology, Shanxi University, Taiyuan, Shanxi 030006, China"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5442b3",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e48e81",
   "metadata": {},
   "source": [
    "- Clustering categorical data is an important task of machine learning, since the type of data widely exists in real world. However, the lack of an inherent order on the domains of categorical features prevents most of classical clustering algorithms from being directly applied for the type of data. Therefore, it is very key issue to learn an appropriate representation of categorical data for the clustering task. In order to address this issue, we develop a categorical data clustering framework based on graph representation.\n",
    "- In this framework, a graph-based representation method for categorical data is proposed, which learns the representation of categorical values from their similar graph to provide similar representations for similar categorical values. We compared the proposed framework with other representation methods for categorical data clustering on benchmark data sets. The experiment results illustrate the proposed frame\u0002work is very effective, compared to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3635a0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8559945",
   "metadata": {},
   "source": [
    "Data clustering is an unsupervised classification technique that aims at grouping a set of unlabeled objects into meaningful clusters, with the requirement that the objects in the same cluster have high similarity but are very dissimilar with objects in other clusters. Clustering techniques have been extensively studied in several communities (e.g., [1] and references therein). Many highquality clustering algorithms, such as k-means [2], density clustering [3], graph clustering [4–6] have been designed for numerical data. Unfortunately, these algorithms cannot be directly applied for clustering of categorical data, where domain values are discrete and have no ordering defined. Since categorical data is ubiquitous in real-world applications [7,8], increasing attentions have been paid to clustering the type of data [9,10]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828e819",
   "metadata": {},
   "source": [
    "Currently, the wide-used methods for clustering categorical data can be categorized into the following two types. (1) Direct-clustering method that designs a clustering algorithm which is suitable for categorical data. Currently, there are three types of categorical data clustering algorithms, seen in [11]. The first type is extension of the numerical data clustering algorithms, such as ROCK [12], k-modes [13] and its variants [14–17], which define the distance measure and the cluster centroids for categorical data. Currently, a number of dissimilarity measures for categorical data have been developed, whose related works can be found in [18]. The second type [19–21] is to employ category utility function [22] as a clustering objective function to maximize the probability that two data objects in the same cluster obtain the same attribute values. The third type [23–25] is to use the information entropy to find out groups of similar objects that have lower entropy than those of dissimilar ones. The advantage of directclustering methods is that their interpretability is strong.They can sufficiently reflect the characteristics of categorical data. Their disadvantage is that their adaptability is weak. For different types of categorical data, we need to design the corresponding algorithms. Besides, most of state-of-the-art clustering algorithm for numerical\n",
    "data can not be directly applied for categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4dd92",
   "metadata": {},
   "source": [
    "(2) Converting-based method that transform categorical data into numerical data and then cluster it by one of the existing clustering algorithms. The two most popular methods are ordinal encoding and one-hot encoding. In ordinal encoding, each categorical value is converted into an integer value. However, the converted numerical data does not necessarily produce meaningful results in the case where categorical domains are not ordered. In one-hot encoding, each category is mapped with a binary variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category. People treat the binary features as numeric in the clustering algorithms [26]. In [27,28], the researchers proposed a link-based representation method which improves the binary features and used the similarity between categorical values, instead of 0. Furthermore, Jian et al. [29,30], Zhu et al. [31] developed the coupled data embedding (CDE) technique to represent categorical data which can capture the couplings between categorical values and clusters. Zheng et al. [32] make use of similarity between objects to define space structure and representation of categorical data. Besides, people make use of a similarity measure to convert the categorical data into a pairwise-similarity matrix which can be used for spectral clustering. For example, the metric learning methods for categorical data have been proposed in [33– 35]. Compared to direct-clustering methods, the converting-based methods can reduce the costs of designing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f742f9",
   "metadata": {},
   "source": [
    "However, its disadvantage is that the converted numerical data often has weak interpretability and may lead to information loss of the original categorical data. In this paper, we focus on converting-based clustering. The effectiveness of categorical data clustering mainly depends on whether they sufficiently make use of the intrinsic similarity of categorical values. However, using numerical vectors to describe the similarity is a challenge for most of converting-based clustering algorithms. In order to solve this problem, we propose a new categorical data clustering framework based on graph representation. This framework divides a categorical data clustering problem into three subproblems: representation of categorical values, representation of objects, and numerical data clustering. Different from existing categorical data representation methods, our main work is to learn and integrate the representation of categorical values from their graph structure, which can sufficiently capture the potential similarity between categorical values and provide the similar numerical representations for similar values. The outline of the rest of this paper is as follows. Section 2 presents a new categorical data clustering framework. Section 3 demonstrates the performance of the proposed framework. Section 4 concludes the paper with some remarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c809fc8",
   "metadata": {},
   "source": [
    "#  Categorical data clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db515759",
   "metadata": {},
   "source": [
    "- 2.1. Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcf778",
   "metadata": {},
   "source": [
    "Let $X$ be a $n \\times m$ data matrix, where $n$ is the number of objects and $m$ is the number of categorical features, $A = \\{a_j\\}^m_{j=1}$ is a set of $m$ categorical features, where $a_j$ is the $j$th feature. $x_i$ is the $i$th row of $X$ which represents the $i$th objects with $m$ feature values. $x_{ij}$ is the $j$th feature value of $x_i$. Each feature $a_j$ describes a domain of values, denoted by $D(a_j)$, associated with a defined semantic and a data type. Here, only consider two general data types, numerical and categorical, and assume other types used in database systems can be mapped to one of the two types. The domains of features associated with the two types are called numerical and categorical, respectively. A numerical domain consists of real numbers. A domain $D(a_j)$ is defined as categorical if it is finite and unordered, i.e., $D(a_j) = \\{a_{j1}, a_{j2}, \\ldots , a_{jnj}\\}$ where $n_j$ is the number of categories of feature $a_j$ for $1 \\leq j \\leq m$. For any $1 \\leq p \\leq q \\leq n_j$, either $a_{jp} = a_{jq}$ or $a_{jp} \\neq a_{jq}$. If each feature in $A$ is categorical, $X$ is called a categorical data set.\n",
    "\n",
    "The aim of clustering categorical data $X$ is to find out a partition of $X$ into $k$ clusters. Let $U$ be a $n \\times k$ partition matrix of $X$, where $u_{il}$ is membership degree of object $x_i$ to the $l$th cluster. Compared to numerical data, the difficulty of clustering categorical data is that we can not visually observe the similarity between categorical values. In order to solve this problem, we study categorical data clustering based on data representation, which transforms the categorical data into numerical data and cluster it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102387d",
   "metadata": {},
   "source": [
    "- 2.2. Clustering framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a965a",
   "metadata": {},
   "source": [
    "In this paper, we propose a new categorical data clustering framework based on graph representation, seen in Fig. 1. Its main idea is to learn a vector for each categorical value and integrate these vectors of the categorical values which an object includes to represent it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817545e",
   "metadata": {},
   "source": [
    "\\begin{table}[]\n",
    "\\begin{tabular}{ll}\n",
    "Table 1                 &          \\\\\n",
    "Set-similarity measures &          \\\\\n",
    "Description             & Equation \\\\\n",
    "Jaccard coefficient     &          \\\\\n",
    "Ochiai coefficient      &          \\\\\n",
    "Overlap coefficient     &          \\\\\n",
    "Dice coefficient        &         \n",
    "\\end{tabular}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9fd21e",
   "metadata": {},
   "source": [
    "Based on this idea, the clustering problem with categorical data representation can be seen to learn three mappings, which are defined as follows:\n",
    "\\begin{itemize}\n",
    "    \\item $f(ajh)$ maps categorical value $ajh$ to a vector with $p$ elements to represent it, for $1 \\leq j \\leq m$ and $1 \\leq h \\leq nj$;\n",
    "    \\item $g(xi) = \\prod_{j=1}^{m} f(xij)$ maps $xi$ to a vector with $q$ elements to represent it, where $\\prod$ is an operation of integrating the vectors of the categorical values of the object;\n",
    "    \\item $\\pi(g(xi)) = [ui1, \\ldots, uik]$ is a clustering function used to map $g(xi)$ to the membership vector of $xi$.\n",
    "\\end{itemize}\n",
    "\n",
    "$f(.)$ and $g(.)$ are used to transform objects into numerical representation. $\\pi(.)$ is used to cluster the transformed data and get its membership matrix $U$. Therefore, we mainly need to discuss how to define $f(.)$ and $g(.)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89939389",
   "metadata": {},
   "source": [
    "- 2.3. Representation of categorical value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cdc33",
   "metadata": {},
   "source": [
    "The one-hot encoding is the most commonly used method to define $f(a_{ij})$, i.e., $f(a_{ij})$ is a binary vector with $n_j$ elements, where the $j$th element is equal to 1 and others are 0. However, the one-hot encoding does not easily reflect the similarity between different categorical values. It can only determine whether two categorical values are the same. In order to solve this problem, we propose a graph-based representation method to get $f(.)$. The main idea of this method is to construct a similarity-relational graph $G$ of all the categorical values and use one of graph embedding methods to learn the representation of nodes in $G$, which is shown in Fig. 2. By a graph embedding method, we can easily capture the inherent similarity between categorical values and find similar representations for similar categorical values.\n",
    "\n",
    "We first construct the graph $G$. Let $G =< V,W >$ be an undirected and weighted graph, where $V = \\bigcup_{j=1}^{m} D(a_j)$ is a set of nodes, $W$ is a $|V| \\times |V|$ weight matrix and $W(a_{jh}, a_{rl})$ is a weight of the edge between nodes $a_{jh}$ and $a_{rl}$ used to reflect their similarity. $W(a_{jh}, a_{rl})$ is computed by measuring the similarity between two object sets $S(a_{jh}) = \\{x_{ij} = a_{jh}, x_i \\in X\\}$ and $S(a_{rl}) = \\{x_{ij} = a_{rl}, x_i \\in X\\}$. We hope $W(a_{jh}, a_{rl})$ is proportional to the number of common objects with the categorical values $a_{jh}$ and $a_{rl}$, i.e.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fe03b",
   "metadata": {},
   "source": [
    "$W(a_{jh}, a_{rl}) \\propto |S(a_{jh}) \\cap S(a_{rl})|$.\n",
    "\n",
    "Based on the assumption, we can employ one of set-similarity measures [36] to define $W$. The representatives of set-similarity measure are shown in Table 1.\n",
    "Given the graph $G$ of categorical values, the graph embedding is employed to find embedding of nodes to $p$-dimensions so that “similar” nodes in the graph have embeddings that are close together. Let $\\phi(W)$ be a graph embedding function which can transform $W$ into a $|V| \\times p$ representation matrix $Z = \\phi(W)$. In this case, we get the representation vector of each categorical value as follows.\n",
    "\n",
    "$f(a_{jh}) = z_r$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa9f04",
   "metadata": {},
   "source": [
    "where $r = \\sum_{i=1}^{j-1} n_i + h$ and $z_r$ is the $r$th row of $Z$. We can employ one of graph embedding methods to get $\\phi(W)$. Currently, a number of graph embedding methods have been developed. Some classical methods, such as Non Embedding (NE) where $W$ is directly seen as a feature data, Spectral Embedding (SE) [4], Nonnegative Matrix Factorization (NMF) [37], Autoencoder (AE) [38], are shown in Table 2. Since the graph embedding operation is implemented on the categorical values and $|V| < n$ in many data sets, its time complexity should be far less than directly learning the representation on a data set.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "953a28ab",
   "metadata": {},
   "source": [
    "\\begin{table}[h]\n",
    "    \\centering\n",
    "    \\caption{Graph embedding methods.}\n",
    "    \\label{tab:graph_embedding_methods}\n",
    "    \\begin{tabular}{|l|l|}\n",
    "        \\hline\n",
    "        \\textbf{Description} & \\textbf{Equation} \\\\\n",
    "        \\hline\n",
    "        NE & $Z = W$ \\\\\n",
    "        SE & $\\min_Z \\operatorname{tr}[Z^T (D - W)Z]$ \\\\\n",
    "        NMF & $\\min_Z ||W - ZH||^2$ \\\\\n",
    "        AE & $\\min_{\\phi, \\psi} ||W - \\psi(\\phi(W))||^2$ \\\\\n",
    "        \\hline\n",
    "    \\end{tabular}\n",
    "\\end{table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc12a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc94e8e",
   "metadata": {},
   "source": [
    "- 2.4. Representation of categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec578f5",
   "metadata": {},
   "source": [
    "Given $f(q_i^n)$ for each categorical value, in order to get $g(x_i)$ to represent objects, we need to define the integration operations $\\mathcal{O}$ which uses the numerical vectors of categorical values of an object to represent it. In this paper, we provide two methods which is shown in Fig. 3 to define $\\mathcal{O}$ as follows.\n",
    "\n",
    "In the first method, $\\mathcal{O}$ is seen as a joint operation and $g(x_i)$ is defined as\n",
    "\n",
    "\\[g(x_i) = [f(x_{i1}), \\ldots , f(x_{im})]\\]\n",
    "\n",
    "which is a $mp$-dimensional vector. If $f(q_i^n)$ is computed by non embedding, i.e., $Z = W$, the squared Euclidean distance between objects is described as\n",
    "\n",
    "\\[d^2(g(x_i), g(x_j)) = \\sum_{h=1}^{m} \\sum_{q_l^v} [W(x_{ih}, q_l^v) - W(x_{jh}, q_l^v)]^2.\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8eac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
